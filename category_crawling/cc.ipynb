{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import regex as re\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "from utils.mysql import MySQL, get_db_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sub(x):\n",
    "    return re.sub(' ','',x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Category_Crawler:\n",
    "    def __init__(self):\n",
    "        self.cat_length = 9\n",
    "        self.cat_dict = dict()\n",
    "        self.web_category = [[['없음','없음','없음','없음','없음']]]*self.cat_length\n",
    "        self.flat_web_category = None\n",
    "        self.cat_names = list()\n",
    "        \n",
    "        self.db = None\n",
    "        self.open_db()\n",
    "        self.db_category = None #list(dict()) 형태\n",
    "        \n",
    "        self.db_cat_df = None\n",
    "        self.web_cat_df = None\n",
    "        self.outer_df = None\n",
    "        \n",
    "        \n",
    "    \n",
    "    def get_category_from_json(self, category_code):\n",
    "        url = 'https://search.shopping.naver.com/search/category?catId={}'.format(category_code)\n",
    "        html = requests.get(url)\n",
    "        time.sleep(0.1)\n",
    "        html = html.text\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "        script = soup.find(\"script\", {\"id\":\"__NEXT_DATA__\"})\n",
    "        category_dict = json.loads(script.next)['props']['pageProps']['initialState']['mainFilters'][0]['filterValues']\n",
    "\n",
    "        result = [[category['title'], category['value']] for category in category_dict]\n",
    "        return result\n",
    "    \n",
    "    \n",
    "    def get_third_tier_with_subgroup(self, first_tier_id):#카테고리 테이블 페이지\n",
    "        result = dict()\n",
    "        cat_names_from_whole_category = list()\n",
    "\n",
    "        url = 'https://search.shopping.naver.com/category/category.nhn?cat_id={}'.format(first_tier_id)\n",
    "        html = requests.get(url)\n",
    "        time.sleep(0.1)\n",
    "        html = html.text\n",
    "        \n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        \n",
    "        \n",
    "        first_regex = re.compile('.category_tit.')\n",
    "        first_tier_name = soup.find(\"h2\", {\"class\" : first_regex}).text.strip()\n",
    "        cat_names_from_whole_category.append([first_tier_name, first_tier_id])\n",
    "        \n",
    "\n",
    "        second_regex = re.compile('.category_cell.')\n",
    "        second_tiers = soup.find_all('div', {'class' : second_regex})\n",
    "        for second_tier in second_tiers:\n",
    "\n",
    "            second_tier_name = re.sub(' ','',second_tier.find('h3').find('strong').text.strip())\n",
    "            second_tier_id = second_tier.find('h3').find('a')['href'].split('catId=')[-1]\n",
    "            cat_names_from_whole_category.append([second_tier_name, second_tier_id])\n",
    "\n",
    "            if second_tier_name == 'PC주변기기': #카테고리 테이블과 제품검색 페이지 카테고리명이 다른경우\n",
    "                second_tier_name = '주변기기'\n",
    "            result[second_tier_name] = dict()\n",
    "            \n",
    "            if second_tier_name in ['노트북', '태블릿PC', '모니터', '마라톤용품', '당구용품', '기타스포츠용품', '제화브랜드']:   # 소분류 없는 중분류들\n",
    "                continue\n",
    "\n",
    "            second_list_regex = re.compile('.category_list.')\n",
    "            for i, child in enumerate(second_tier.find('ul', {'class' : second_list_regex}).children):\n",
    "                if child.find('a') != -1:\n",
    "                    third_tier = child\n",
    "                    third_tier_name = re.sub(' ','',child.find('a').text.strip())\n",
    "                    third_tier_id = child.find('a')['href'].split('catId=')[-1]\n",
    "                    cat_names_from_whole_category.append([third_tier_name, third_tier_id])\n",
    "\n",
    "                    if third_tier.find('ul'):\n",
    "                        result[second_tier_name][third_tier_name] = {'id':third_tier_id}\n",
    "                        \n",
    "                        for fourth_tier_1 in third_tier.find_all('ul'):     #more_on과 일반적인 리스트가 동시에 있는 경우가 있음\n",
    "                            for fourth_tier in fourth_tier_1.find_all('li'):#more_on과 일반적인 리스트가 동시에 있는 경우가 있음\n",
    "                                fourth_tier_name = fourth_tier.text.strip()\n",
    "                                fourth_tier_id = fourth_tier.find('a')['href'].split('catId=')[-1]\n",
    "                                cat_names_from_whole_category.append([fourth_tier_name, fourth_tier_id])\n",
    "                                #print(fourth_tier_name)\n",
    "                        \n",
    "        for k in list(result.items()):\n",
    "            if len(k[1]) == 0 :\n",
    "                result.pop(k[0])\n",
    "                \n",
    "        self.cat_names.extend(cat_names_from_whole_category)\n",
    "        return result\n",
    "    \n",
    "    \n",
    "    def get_category_table(self, first_tier_id):#상품 검색 페이지\n",
    "        result = list()\n",
    "        url = 'https://search.shopping.naver.com/search/category?catId={}'.format(first_tier_id)\n",
    "        html = requests.get(url)\n",
    "        time.sleep(0.1)\n",
    "        html = html.text\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "        first_regex = re.compile('.category_info.')\n",
    "        first_tier_name = soup.find(\"div\", {\"class\":first_regex}).text.strip()\n",
    "        result.append([first_tier_name, None, None, None, first_tier_id])\n",
    "\n",
    "        subgroup_dict = self.get_third_tier_with_subgroup(first_tier_id)\n",
    "\n",
    "        second_tier = self.get_category_from_json(first_tier_id)\n",
    "        for second in tqdm(second_tier, desc=' id:{}, name:{}    '.format(first_tier_id, first_tier_name)):\n",
    "            second[0] = re.sub(' ','',second[0])\n",
    "            result.append([first_tier_name, second[0], None, None, second[1]])\n",
    "            if second[0] in ['노트북', '태블릿PC', '모니터', '마라톤용품', '당구용품', '기타스포츠용품', '제화브랜드']:   # 소분류 없는 중분류들\n",
    "                continue\n",
    "\n",
    "            third_tier = self.get_category_from_json(second[1])\n",
    "            for third in third_tier:\n",
    "                third[0] = re.sub(' ','',third[0])\n",
    "                result.append([first_tier_name, second[0], third[0], None, third[1]])\n",
    "\n",
    "                if second[0] in subgroup_dict.keys():\n",
    "                    if third[0] in subgroup_dict[second[0]]:\n",
    "                        #print(third[0])\n",
    "                        fourth_tier = self.get_category_from_json(subgroup_dict[second[0]][third[0]]['id'])\n",
    "                        for fourth in fourth_tier:\n",
    "                            result.append([first_tier_name, second[0], third[0], fourth[0], fourth[1]])\n",
    "        return result\n",
    "    \n",
    "    def update_cat_dict(self, start_code, end_code):\n",
    "        code_list = [i for i in range(start_code, end_code)]\n",
    "        while(len(code_list) > 0):\n",
    "            try:\n",
    "                for i in range(code_list[0], code_list[-1]+1):\n",
    "                    category_table = self.get_category_table('{}'.format(i))\n",
    "                    self.web_category[i-50000000] = category_table\n",
    "                    for data in category_table:\n",
    "                        if data[0] and data[1] == None:\n",
    "                            self.cat_dict[data[0]] = {'name' : data[0], 'id':data[4], 'parent':None}\n",
    "\n",
    "                        if data[1] and data[2] == None:\n",
    "                            self.cat_dict[data[0]][data[1]] = {'name' : data[1], 'id':data[4], 'parent':data[0]}\n",
    "\n",
    "                        if data[2] and data[3] == None:\n",
    "                            self.cat_dict[data[0]][data[1]][data[2]] = {'name':data[2], 'id':data[4], 'parent':data[1]}\n",
    "\n",
    "                        if data[3]:\n",
    "                            self.cat_dict[data[0]][data[1]][data[2]][data[3]] = {'name':data[3], 'id':data[4], 'parent':data[2]}\n",
    "\n",
    "                    code_list.remove(i)\n",
    "            \n",
    "            except Exception as e:\n",
    "                print('Connection Failed, Retry...', e)\n",
    "        self.get_flat_web_cat()\n",
    "        \n",
    "    def get_flat_web_cat(self):\n",
    "        self.flat_web_category = [y for x in self.web_category for y in x]#flatten\n",
    "        \n",
    "    def save_cat_dict_to_json(self, path = 'category_crawling/category.json'):\n",
    "        with open(path, 'w') as f:\n",
    "            json.dump(self.cat_dict, f)\n",
    "    \n",
    "    \n",
    "    def open_db(self):\n",
    "        host, db_name, user, password = get_db_info()\n",
    "        self.db = MySQL(host, db_name, user, password)\n",
    "        \n",
    "    def get_db_category_table(self):\n",
    "        self.db_category = self.db.execute('select sc.id, sc.category_code, cl.name                                     from zeliterai.site_category sc                                     join zeliterai.category_language cl on sc.category_id = cl.category_id                                     where sc.site_id = 3 order by sc.category_code')\n",
    "        \n",
    "    def get_category_dataframe(self):\n",
    "        self.get_db_category_table()\n",
    "        self.db_cat_df = pd.DataFrame([[d['name'], d['category_code']] for d in self.db_category], columns=['name', 'id'])\n",
    "        self.web_cat_df = pd.DataFrame(self.flat_web_category, columns=['1','2','3','4','id'])\n",
    "        \n",
    "    def get_duplicted_web_category(self):\n",
    "        return self.web_cat_df[self.web_cat_df['id'].duplicated(keep=False)].sort_values(['id'])\n",
    "    \n",
    "    def find_category_from_web(self, category_code):\n",
    "        return self.web_cat_df[self.web_cat_df['id'] == '{}'.format(category_code)]\n",
    "    \n",
    "    def find_category_from_db(self, category_code):\n",
    "        return self.db_cat_df[self.db_cat_df['id'] == '{}'.format(category_code)]\n",
    "    \n",
    "    def get_web_db_outer_join(self):\n",
    "        flat_cat_list = list()\n",
    "        for c in self.flat_web_category:\n",
    "            if c[3] != None:\n",
    "                flat_cat_list.append([c[3],c[4]])\n",
    "            if c[2] and c[3] == None:\n",
    "                flat_cat_list.append([c[2],c[4]])\n",
    "            if c[1] and c[2] == None:\n",
    "                flat_cat_list.append([c[1],c[4]])\n",
    "            if c[0] and c[1] == None:\n",
    "                flat_cat_list.append([c[0],c[4]])\n",
    "                \n",
    "        flat_web_cat_df = pd.DataFrame(flat_cat_list, columns=['name', 'id'])\n",
    "        \n",
    "        self.outer_df = pd.merge(flat_web_cat_df, self.db_cat_df, left_on='id', right_on='id', how='outer')\n",
    "        self.outer_df.columns = ['naver', 'category_code', 'db']\n",
    "        self.outer_df = self.outer_df[self.outer_df['naver'].isna() | self.outer_df['db'].isna()].fillna('없음')\n",
    "        return self.outer_df\n",
    "    \n",
    "    def save_outer_df_to_csv(self, path='category_crawling/outer.csv'):\n",
    "        self.outer_df.to_csv(path, index_label=False, index=False)\n",
    "        \n",
    "        \n",
    "        \n",
    "    def validate(self, path = 'category_crawling/참고.csv'):\n",
    "        tmp_list = list()\n",
    "        cat_df = pd.DataFrame(self.cat_names, columns=['name', 'id'])\n",
    "        ids = cat_df['id'].values.tolist()\n",
    "        \n",
    "        for t in self.web_cat_df['id'].values.tolist():\n",
    "            if (t not in ids) and t != '없음':\n",
    "                tmp_list.append(t)\n",
    "                \n",
    "        result = sorted(list(set(tmp_list)))\n",
    "        self.web_cat_df[self.web_cat_df['id'].isin(result)].to_csv(path,index=False)\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " id:50000000, name:패션의류    : 100%|██████████| 4/4 [00:06<00:00,  1.65s/it]\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    cc = Category_Crawler()\n",
    "    cc.update_cat_dict(50000000, 50000001)\n",
    "    \n",
    "    directory = os.path.join('category_crawling')\n",
    "    if not os.path.isdir(directory):\n",
    "        os.mkdir(directory)\n",
    "        \n",
    "    cc.save_cat_dict_to_json()\n",
    "    cc.get_category_dataframe()\n",
    "    \n",
    "    cc.get_web_db_outer_join()\n",
    "    cc.save_outer_df_to_csv()\n",
    "    cc.validate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
